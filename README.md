# Placements-Statistics

## Descriptive Statistics
### ---> Mean, Median, Mode: Measures of central tendency.
### ---> Variance and Standard Deviation: Measures of dispersion.
### ---> Quartiles and Percentiles: Understanding distribution spread.
### ---> Skewness and Kurtosis: Measuring the asymmetry and peakedness of data.

## Probability Theory
### ---> Basic Probability Concepts: Events, sample space, conditional probability, independence.
### ---> Bayes Theorem: Used for computing posterior probabilities and in models like Naive Bayes.
### ---> Random Variables: Discrete and continuous variables, probability distributions.
### ---> Joint, Marginal, and Conditional Distributions: Understanding how variables interact.

## Probability Distributions
### ---> Normal Distribution (Gaussian): Bell curve, properties, Z-scores, the Central Limit Theorem.
### ---> Binomial Distribution: Discrete distribution for binary outcomes (success/failure).
### ---> Poisson Distribution: Discrete distribution for counting events over time/space.
### ---> Exponential Distribution: Modeling time between events in a Poisson process.
### ---> t-Distribution: Used when sample size is small or population variance is unknown.
### ---> Uniform Distribution: Probability distribution where all outcomes are equally likely.

## Inferential Statistics
### ---> Hypothesis Testing: Null and alternative hypotheses, p-values, type I and II errors.
### ---> Confidence Intervals: Estimating population parameters.
### ---> t-Test: Testing the means of small sample datasets.
### ---> ANOVA (Analysis of Variance): Comparing means of multiple groups.
### ---> Chi-Square Test: Testing relationships between categorical variables.
### ---> Z-Test: Used when population variance is known and sample size is large.

## Correlation and Covariance
### ---> Pearson Correlation: Measuring linear relationship between two variables.
### ---> Spearman Rank Correlation: Measuring monotonic relationships between two variables.
### ---> Covariance: Measuring how two variables move together.
### ---> Correlation vs. Causation: Understanding the difference between association and cause-effect.

## Regression Analysis
### ---> Simple Linear Regression: Fitting a line to data, interpreting slope and intercept.
### ---> Multiple Linear Regression: Fitting a linear relationship to multiple features.
### ---> Assumptions of Regression: Linearity, independence, homoscedasticity, normality.
### ---> R-squared and Adjusted R-squared: Goodness of fit metrics for regression models.
### ---> p-values and Coefficients: Understanding significance of model features.

## Sampling Methods
### ---> Random Sampling: Ensuring every individual in the population has an equal chance of selection.
### ---> Stratified Sampling: Dividing the population into subgroups and sampling within each.
### ---> Bootstrapping: Resampling method used to estimate the accuracy of sample statistics.
### ---> Law of Large Numbers: Larger sample sizes tend to produce results closer to the population mean.

## Central Limit Theorem
### ---> CLT Importance: Foundation for making inferences about population means using sample data.
### ---> Normal Approximation: Understanding how sample means are normally distributed, even for non-normal populations.

## Statistical Significance
### ---> p-value: Understanding what it indicates and how to interpret it.
### ---> Significance Levels (alpha): Choosing an appropriate threshold for rejecting the null hypothesis.
### ---> Power of a Test: The probability of correctly rejecting the null hypothesis when it is false.

## Bayesian Statistics
### ---> Bayesian Inference: Updating the probability of a hypothesis as more evidence becomes available.
### ---> Priors and Posteriors: Using prior beliefs and new evidence to form updated beliefs.
### ---> Markov Chain Monte Carlo (MCMC): Sampling from complex distributions in Bayesian analysis.

## Resampling Techniques
### ---> Cross-Validation: Splitting data into training and testing sets multiple times to evaluate model performance.
### ---> Bootstrapping: Randomly sampling from the dataset with replacement to assess the stability of a statistic.

## Experimental Design
### ---> A/B Testing: Designing experiments to compare two treatments (e.g., web page layouts, marketing campaigns).
### ---> Control Groups and Randomization: Ensuring unbiased experiments.
### ---> Factorial Designs: Studying the effect of two or more factors in an experiment.

### Online Resources to Study in Depth:
### ---> [Introduction to Descriptive Statistics - Towards Data Science](https://towardsdatascience.com/intro-to-descriptive-statistics-and-probability-for-data-science-8effec826488)
### ---> [A Practical Guide to Hypothesis Testing - Towards Data Science](https://towardsdatascience.com/understanding-hypothesis-testing-65f9b3e9ab1f)
### ---> [Linear Regression Explained - Towards Data Science](https://towardsdatascience.com/linear-regression-detailed-view-ea73175f6e86)
### ---> [Common Probability Distributions - Towards Data Science](#)
### ---> [Bayesian Statistics Explained - Towards Data Science](#)
